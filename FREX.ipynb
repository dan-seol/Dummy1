{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Using cached https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.8.2 (from scipy)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy, scipy\n",
      "Successfully installed numpy-1.16.1 scipy-1.2.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install rpy2\n",
    "####\n",
    "# Non-Exported Utility Functions\n",
    "####\n",
    "\n",
    "#' Calculate FREX (FRequency and EXclusivity) Words\n",
    "#' \n",
    "#' A primarily internal function for calculating FREX words.\n",
    "#' We expect most users will use \\code{\\link{labelTopics}} instead.\n",
    "#' \n",
    "#' FREX attempts to find words which are both frequent in and exclusive to a topic of interest.\n",
    "#' Balancing these two traits is important as frequent words are often by themselves simply functional\n",
    "#' words necessary to discuss any topic.  While completely exclusive words can be so rare as to not\n",
    "#' be informative. This accords with a long-running trend in natural language processing which is best exemplified\n",
    "#' by the Term frequency-Inverse document frequency metric.  \n",
    "#' \n",
    "#' Our notion of FREX comes from a paper by Bischof and Airoldi (2012) which proposed a Hierarchical\n",
    "#' Poisson Deconvolution model.  It relies on a known hierarchical structure in the documents and requires\n",
    "#' a rather complicated estimation scheme.  We wanted a metric that would capture their core insight but\n",
    "#' still be fast to compute.\n",
    "#' \n",
    "#' Bischof and Airoldi consider as asummary for a word's contribution to a topic the harmonic mean of the\n",
    "#' word's rank in terms of exclusivity and frequency.  The harmonic mean is attractive here because it \n",
    "#' does not allow a high rank along one of the dimensions to compensate for the lower rank in another. Thus\n",
    "#' words with a high score must be high along both dimensions.\n",
    "#' \n",
    "#' The formula is ' \n",
    "#'\\deqn{FREX = \\left(\\frac{w}{F} + \\frac{1-w}{E}\\right)^{-1}}{FREX = ((w/F) + ((1-w)/E))^-1} \n",
    "#' where F is the frequency score given by the emperical CDF of the word in it's topic distribution.  Exclusivity\n",
    "#' is calculated by column-normalizing the beta matrix (thus representing the conditional probability of seeing\n",
    "#' the topic given the word).  Then the empirical CDF of the word is computed within the topic.  Thus words with\n",
    "#' high values are those where most of the mass for that word is assigned to the given topic.\n",
    "#' \n",
    "#' For rare words exclusivity will always be very high because there simply aren't many instances of the word.\n",
    "#' If \\code{wordcounts} are passed, the function will calculate a regularized form of this distribution using a\n",
    "#' James-Stein type estimator described in \\code{\\link{js.estimate}}.\n",
    "#' \n",
    "#' @param logbeta a K by V matrix containing the log probabilities of seeing word v conditional on topic k\n",
    "#' @param w a value between 0 and 1 indicating the proportion of the weight assigned to frequency \n",
    "#' @param wordcounts a vector of word counts.  If provided, a James-Stein type shrinkage estimator is \n",
    "#' applied to stabilize the exclusivity probabilities. This helps with the concern that the rarest words\n",
    "#' will always be completely exclusive.\n",
    "#' @references \n",
    "#' Bischof and Airoldi (2012) \"Summarizing topical content with word frequency and exclusivity\"\n",
    "#' In Proceedings of the International Conference on Machine Learning.\n",
    "#' @seealso \\code{\\link{labelTopics}} \\code{\\link{js.estimate}}\n",
    "#' @export\n",
    "#' @keywords internal\n",
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datatable==0.8.0 from https://s3.amazonaws.com/h2o-release/datatable/stable/datatable-0.8.0/datatable-0.8.0-cp36-cp36m-linux_x86_64.whl\n",
      "  Using cached https://s3.amazonaws.com/h2o-release/datatable/stable/datatable-0.8.0/datatable-0.8.0-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting blessed (from datatable==0.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/3f/96/1915827a8e411613d364dd3a56ef1fbfab84ee878070a69c21b10b5ad1bb/blessed-1.15.0-py2.py3-none-any.whl\n",
      "Collecting typesentry>=0.2.6 (from datatable==0.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/0f/37/3757249f05aac8a08d9742f9a35c17ab6895eb916b83bbf3a23eae6842b2/typesentry-0.2.7-py2.py3-none-any.whl\n",
      "Collecting six>=1.9.0 (from blessed->datatable==0.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting wcwidth>=0.1.4 (from blessed->datatable==0.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\n",
      "Collecting colorama>=0.3 (from typesentry>=0.2.6->datatable==0.8.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
      "Installing collected packages: six, wcwidth, blessed, colorama, typesentry, datatable\n",
      "Successfully installed blessed-1.15.0 colorama-0.4.1 datatable-0.8.0 six-1.12.0 typesentry-0.2.7 wcwidth-0.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip3 install https://s3.amazonaws.com/h2o-release/datatable/stable/datatable-0.8.0/datatable-0.8.0-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import special as sp\n",
    "from scipy import stats as ss\n",
    "import datatable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_col(arr, col):\n",
    "    return map(lambda x : x[col], arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Accurately computes the logarithm of the sum of exponentials, that is,\n",
    "\n",
    "#' \\eqn{log(sum(exp(lx)))}.\n",
    "\n",
    "#It is a smooth approximation of max[\\vec{x}_j]\n",
    "\n",
    "def col_lse(mat): \n",
    "    return sp.logsumexp(mat, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.55144471 1.55144471 1.55144471]\n",
      "[  4.40760596   7.16984602 111.        ]\n",
      "[ 1.99887605e+00  8.00000000e+01  3.00000000e+03  1.90483244e+00\n",
      " -5.59810301e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Testing col_lse (matches with R output)\n",
    "print(col_lse(np.identity(3)))\n",
    "print(col_lse(np.array([[2,3,4],[4,5,7],[10,11,111]]).T))\n",
    "print(col_lse(np.array([[.7, .5, -70,0,1],[0,5,80,1,1],[2.7,3000,-7,0,-5000],[0,0,0,1,0],[-1,-2,-3,-4,-58]]).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''js.estimate <- function(prob, ct) {\n",
    "  if(ct<=1) {\n",
    "    #basically if we only observe a count of 1\n",
    "    #the variance goes to infinity and we get the uniform distribution.\n",
    "    return(rep(1/length(prob), length(prob)))\n",
    "  }\n",
    "  # MLE of prob estimate\n",
    "  mlvar <- prob*(1-prob)/(ct-1)\n",
    "  unif <- rep(1/length(prob), length(prob)) \n",
    "  \n",
    "  # Deviation from uniform\n",
    "  deviation <- sum((prob-unif)^2)\n",
    "  \n",
    "  #take care of special case,if no difference it doesn't matter\n",
    "  if(deviation==0) return(prob)\n",
    "  \n",
    "  lambda <- sum(mlvar)/deviation\n",
    "  #if despite  our best efforts we ended up with an NaN number-just return the uniform distribution.\n",
    "  if(is.nan(lambda)) return(unif)\n",
    "  \n",
    "  #truncate\n",
    "  if(lambda>1) lambda <- 1\n",
    "  if(lambda<0) lambda <- 0\n",
    "  \n",
    "  #Construct shrinkage estimator as convex combination of the two\n",
    "  lambda*unif + (1 - lambda)*prob\n",
    "}\n",
    "'''\n",
    "#JSestimate: np.array representation of probability distribution -> np.array representation of estimated prob\n",
    "\n",
    "\n",
    "def JSestimate(prob, ct):\n",
    "    #basically if we only observe a count of 1\n",
    "    #the variance goes to infinity and we get the uniform distribution.\n",
    "    if (ct <= 1):\n",
    "         \n",
    "        return (1.0/np.shape(prob))*np.ones(np.shape(prob))\n",
    "    #Find the MLE for prob estimate\n",
    "    mlvar = prob*(1-prob)/(ct-1)\n",
    "    unif = (1.0/np.shape(prob))*np.ones(np.shape(prob))\n",
    "    \n",
    "    #We measure the deviation from uniform\n",
    "    dev = sum((prob-unif)**2)\n",
    "    if (dev == 0):\n",
    "        return prob\n",
    "    xi = sum(mlvar)/dev\n",
    "      #if despite  our best efforts we ended up with an NaN number-just return the uniform distribution.\n",
    "    if (np.isnan(xi)): \n",
    "        return(unif)\n",
    "    #Truncate\n",
    "    if (xi>1):\n",
    "        xi = 1\n",
    "    if (xi < 0):\n",
    "        xi = 0\n",
    "    #Construct shrinkage estimator as convex combination of the two\n",
    "    return (xi*unif) + (1-xi)*prob     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 1., 2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([1,2,3])-np.ones((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding safelog()\n",
    "#R code-\n",
    "\n",
    "'''\n",
    "\n",
    "safelog <- function(x, min=-1000) {\n",
    " out <- log(x)\n",
    " out[which(out<min)] <- min\n",
    " out\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "def safelog(x, min =-1000):\n",
    "    output = np.log(x)\n",
    "    output[np.where(output<min)] = min\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "'''\n",
    "calcfrex <- function(logbeta, w=.5, wordcounts=NULL) {\n",
    "  excl <- t(t(logbeta) - col.lse(logbeta))\n",
    "  if(!is.null(wordcounts)) {\n",
    "    #if word counts provided calculate the shrinkage estimator\n",
    "    excl <- safelog(sapply(1:ncol(excl), function(x) js.estimate(exp(excl[,x]), wordcounts[x])))\n",
    "  } \n",
    "  freqscore <- apply(logbeta,1,data.table::frank)/ncol(logbeta)\n",
    "  exclscore <- apply(excl,1,data.table::frank)/ncol(logbeta)\n",
    "  frex <- 1/(w/freqscore + (1-w)/exclscore)\n",
    "  apply(frex,2,order,decreasing=TRUE)\n",
    "}\n",
    "'''    \n",
    "\n",
    "def calcfrex(logbeta, w = .5, wordcounts=None):\n",
    "    excl = (logbeta.T -col_lse(logbeta) ).T\n",
    "    ncol_excl = len(excl[0])\n",
    "    ncol= len(logbeta[0])\n",
    "    if wordcounts is not None:\n",
    "        #if word counts provided calculate the shrinkage estimator\n",
    "        \n",
    "        exclT= excl.T\n",
    "        for i in range(ncol):\n",
    "            exclT[i] = safelog(np.apply_along_axis(JSestimate,1,np.exp(exclT[i])), wordcounts[i])\n",
    "        #wordcounts[x]?\n",
    "        excl= exclT.T\n",
    "    freqscore = np.apply_along_axis(ss.rankdata,1,logbeta)/ncol\n",
    "    exclscore = np.apply_along_axis(ss.rankdata,1,excl)/ncol_excl\n",
    "    frex = 1.0/(w/freqscore+(1-w)/exclscore)\n",
    "    np.apply_along_axis(argsort, 0, -1*frex)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.array([1,4,3,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corruption dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
